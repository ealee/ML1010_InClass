{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  not bother think would see movie great supspen...  negative\n",
      "1  careful one get mitt change way look kung fu f...  positive\n",
      "2  chili palmer tired movie know want success mus...  negative\n",
      "3  follow little know 1998 british film make budg...  positive\n",
      "4  dark angel cross huxley brave new world percys...  positive\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'movie_reviews_cleaned.csv')\n",
    "\n",
    "# take a peek at the data\n",
    "print(dataset.head())\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "\n",
    "# normalize datasets\n",
    "#norm_train_reviews = train_reviews#tn.normalize_corpus(train_reviews)\n",
    "#norm_test_reviews = test_reviews#tn.normalize_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Text Classification Pipeline with The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(train_reviews)\n",
    "# build Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(cv_train_features, train_sentiments)\n",
    "\n",
    "# Build Text Classification Pipeline\n",
    "lr_pipeline = make_pipeline(cv, lr)\n",
    "\n",
    "# save the list of prediction classes (positive, negative)\n",
    "classes = list(lr_pipeline.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyze Model Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipeline.predict(['the lord of the rings is an excellent movie', \n",
    "                     'i hated the recent movie on tv, it was so bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.162039</td>\n",
       "      <td>0.837961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.254386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  positive\n",
       "0  0.162039  0.837961\n",
       "1  0.745614  0.254386"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lr_pipeline.predict_proba(['the lord of the rings is an excellent movie', \n",
    "                     'i hated the recent movie on tv, it was so bad']), columns=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Model Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skater'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e45e5360a2bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskater\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_interpretation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skater'"
     ]
    }
   ],
   "source": [
    "import skater\n",
    "from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
    "#https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=classes)\n",
    "def interpret_classification_model_prediction(doc_index, norm_corpus, corpus, \n",
    "                                              prediction_labels, explainer_obj):\n",
    "    # display model prediction and actual sentiments\n",
    "    print(\"Test document index: {index}\\nActual sentiment: {actual}\\nPredicted sentiment: {predicted}\"\n",
    "      .format(index=doc_index, actual=prediction_labels[doc_index],\n",
    "              predicted=lr_pipeline.predict([norm_corpus[doc_index]])))\n",
    "    # display actual review content\n",
    "    print(\"\\nReview:\", corpus[doc_index])\n",
    "    # display prediction probabilities\n",
    "    print(\"\\nModel Prediction Probabilities:\")\n",
    "    for probs in zip(classes, lr_pipeline.predict_proba([norm_corpus[doc_index]])[0]):\n",
    "        print(probs)\n",
    "    # display model prediction interpretation\n",
    "    exp = explainer.explain_instance(norm_corpus[doc_index], \n",
    "                                     lr_pipeline.predict_proba, num_features=10, \n",
    "                                     labels=[1])\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpret_classification_model_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d0c7e0dc64a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_reviews,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                          \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          explainer_obj=explainer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interpret_classification_model_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "doc_index = 100 \n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_reviews,\n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpret_classification_model_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e543a376ddde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                          \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          explainer_obj=explainer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interpret_classification_model_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "doc_index = 2000\n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interpret_classification_model_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1cd175b2566a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m347\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                          \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_sentiments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          explainer_obj=explainer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interpret_classification_model_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "doc_index = 347 \n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
